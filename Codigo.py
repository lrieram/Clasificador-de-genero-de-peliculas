# -*- coding: utf-8 -*-
"""TP2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-OisZYndAD5-rQdVuNHZGQSwkw7HhNJ0
"""

import numpy as np
import pandas as pd
from statistics import mode
import seaborn as sns
from matplotlib import pyplot as plt

"""#Metodo de la potencia con deflacion"""

# Instalación de Pybind11 y Eigen, clonó el repo con el código del método de la potencia en C++, compilación e importación del módulo powerMethod

!pip show pybind11
!rm -r tp2-metodos-numericos/
!rm -rf eigen
#!apt-get update
!apt-get install -y libeigen3-dev python3-dev build-essential
!pip install pybind11
!git clone https://github.com/lrieram/Metodo_de_la_potencia.git

#!sed -i 's|#include <Eigen/Dense>|#include <eigen3/Eigen/Dense>|' ./tp2-metodos-numericos/powerMethod.cpp
!g++ -O3 -Wall -shared -std=c++11 -fPIC -I/usr/include/eigen3 -I/usr/include/python3.10 -I/usr/local/lib/python3.10/dist-packages/pybind11/include ./Metodo_de_la_potencia/powerMethod.cpp -o powerMethod.cpython-310-x86_64-linux-gnu.so

import powerMethod

"""##Test con autovalores enteros"""

# TEST CON AUTOVALORES ENTEROS

rango_min = 1
rango_max = 100

for i in range(2,100):

  eigenvalues = np.random.choice(np.arange(rango_min, rango_max + 1), size=i, replace=False)

  D = np.diag(eigenvalues)

  # Definir el rango y cantidad de elementos deseados
  #n = i  # Número de elementos en el vector

  v = np.ones((D.shape[0], 1))

  v = v / np.linalg.norm(v)

  # Matriz de Householder
  B = np.eye(D.shape[0]) - 2 * (v @ v.T)

  # Matriz a diagonalizar, recordar B es simétrica y ortogonal
  M = B @ D @ B.T

  result = powerMethod.eigen(M, i, 30000, 1e-50)
  autovalorDominante = max(eigenvalues)
  indiceAutovalorDominante = np.where(eigenvalues == autovalorDominante)[0][0]
  assert( np.isclose ( np.abs(result[0][0]), np.abs(max(eigenvalues)) )  )
  assert(np.allclose(np.abs(result[1][:, 0]), np.abs(B[:, indiceAutovalorDominante])))

"""##Test con autovalores floats"""

# TEST CON AUTOVALORES FLOATS

for i in range(2,100):

  eigenvalues_float = vector_floats = np.random.uniform(0.0, 100.0, i)
  D = np.diag(eigenvalues)

  v = np.ones((D.shape[0], 1))

  v = v / np.linalg.norm(v)

  # Matriz de Householder
  B = np.eye(D.shape[0]) - 2 * (v @ v.T)

  # Matriz a diagonalizar, recordar B es simétrica y ortogonal
  M = B @ D @ B.T

  result = powerMethod.eigen(M, i, 30000, 1e-20)
  autovalorDominante = max(eigenvalues)
  indiceAutovalorDominante = np.where(eigenvalues == autovalorDominante)[0][0]
  assert( np.isclose ( np.abs(result[0][0]), np.abs(max(eigenvalues)) )  )
  assert(np.allclose(np.abs(result[1][:, 0]), np.abs(B[:, indiceAutovalorDominante])))

"""##Test con autovalores repetidos"""

#Hasta 100 tarda una banda (5hs aprox) Pero funciona
for i in range(3,100):

  numeros_unicos = np.random.choice(np.arange(rango_min, rango_max + 1), size=i-1, replace=False)
  numero_repetido = np.random.choice(numeros_unicos, size=1)

  eigenvalues_repetidos = np.append(numeros_unicos, numero_repetido)
  np.random.shuffle(eigenvalues_repetidos)

  D = np.diag(eigenvalues_repetidos)

  v = np.ones((D.shape[0], 1))

  v = v / np.linalg.norm(v)

  B = np.eye(D.shape[0]) - 2 * (v @ v.T)

  # Matriz a diagonalizar
  M = B @ D @ B.T

  result = powerMethod.eigen(M, i, 10000000, 1e-40)
  for j in range(len(eigenvalues_repetidos)):

    Mv = M @ result[1][:, j]
    lambdav = result[0][j] * result[1][:, j]
    aproximationToNullVector =  np.abs(Mv - lambdav)
    assert(np.allclose(np.abs(aproximationToNullVector[:]), np.abs(np.zeros(M.shape[0]))))

"""##Medida del error y la velocidad de convergencia"""

epsilons = np.logspace(-4,0,100) # 100 valores log-espaciados de epsilon

pasosParaConvergerPromedios = []
pasosParaConvergerDesviaciones = []
erroresPromedios = []
erroresDesviaciones = []
n = 5

for e in epsilons:
  errores = []
  pasosParaConverger = []
  for i in range(100):

    # Creo la matriz de Householder
    D = np.diag([10, 10-e, 5, 2, 1])
    v = np.random.rand(n)
    v = v / np.linalg.norm(v)

    H = np.eye(n) - 2 * np.outer(v, v)

    # Creo la matriz a diagonalizar
    A = H @ D @ H.T

    result = powerMethod.powerIteration(A, 50000, 1e-7)
    #print(result)
    autovalorDominante = result[0]
    autovectorDominante = result[1]
    pasos = result[2]
    pasosParaConverger.append(pasos)
    error = np.linalg.norm(A@autovectorDominante - autovalorDominante*autovectorDominante)
    errores.append(error)

  # Calculo el promedio y el desvío estándar de los errores
  promedioErrores = np.mean(errores)
  desvioEstandarErrores = np.std(errores)
  erroresPromedios.append(promedioErrores)
  erroresDesviaciones.append(desvioEstandarErrores)

  promedioPasos = np.mean(pasosParaConverger)
  desvioEstandarPasos = np.std(pasosParaConverger)
  pasosParaConvergerPromedios.append(promedioPasos)
  pasosParaConvergerDesviaciones.append(desvioEstandarPasos)

plt.figure(figsize=(14, 5))

# Primer gráfico: Errores
plt.subplot(1, 2, 1)
plt.errorbar(epsilons, erroresPromedios, yerr=erroresDesviaciones, fmt='o',
             capsize=5, elinewidth=2, markerfacecolor='red',
             markeredgecolor='black', markersize=8, label='Errores')

plt.xscale('log')
plt.yscale('log')
plt.xlabel('Epsilon')
plt.ylabel('Promedio de Errores')
plt.title('Promedio de Errores en función de Epsilon')
plt.grid(True)
plt.legend()

# Segundo gráfico: Pasos de convergencia
plt.subplot(1, 2, 2)  # 1 fila, 2 columnas, gráfico 2
plt.errorbar(epsilons, pasosParaConvergerPromedios, yerr=pasosParaConvergerDesviaciones, fmt='o',
             capsize=5, elinewidth=2, markerfacecolor='green',
             markeredgecolor='black', markersize=8, label='Pasos de Convergencia')

plt.xscale('log')
plt.yscale('linear')
plt.xlabel('Epsilon')
plt.ylabel('Promedio de Pasos')
plt.title('Promedio de Pasos de Convergencia en función de Epsilon')
plt.grid(True)
plt.legend()

# Mostrar la figura
plt.tight_layout()  # Ajusta el espaciado entre subgráficos
plt.show()

epsilons = np.logspace(-4,0,100) # 100 valores log-espaciados de epsilon

pasosParaConvergerPromedios = [[],[],[],[],[]]
pasosParaConvergerDesviaciones = [[],[],[],[],[]]
erroresPromedios = [[],[],[],[],[]]
erroresDesviaciones = [[],[],[],[],[]]
n = 5

for e in epsilons:
    errores = [[],[],[],[],[]]
    pasosParaConverger = [[],[],[],[],[]]
    for i in range(100):

        # Creo la matriz de Householder
        D = np.diag([10, 10-e, 5, 2, 1])
        v = np.random.rand(n)
        v = v / np.linalg.norm(v)

        H = np.eye(n) - 2 * np.outer(v, v)

        # Creo la matriz a diagonalizar
        A = H @ D @ H.T
        A_nueva = A.copy()
        for j in range(5):
            result = powerMethod.powerIteration(A_nueva, 50000, 1e-7)
            #print(result)
            autovalorDominante = result[0]
            autovectorDominante = result[1]
            pasos = result[2]
            pasosParaConverger[j].append(pasos)
            error = np.linalg.norm(A@autovectorDominante - autovalorDominante*autovectorDominante)
            errores[j].append(error)
            A_nueva = A_nueva - autovalorDominante*np.outer(autovectorDominante,autovectorDominante)

  # Calculo el promedio y el desvío estándar de los errores
    for j in range(5):
        promedioErrores = np.mean(errores[j])
        desvioEstandarErrores = np.std(errores[j])
        erroresPromedios[j].append(promedioErrores)
        erroresDesviaciones[j].append(desvioEstandarErrores)

        promedioPasos = np.mean(pasosParaConverger[j])
        desvioEstandarPasos = np.std(pasosParaConverger[j])
        pasosParaConvergerPromedios[j].append(promedioPasos)
        pasosParaConvergerDesviaciones[j].append(desvioEstandarPasos)

fig, axs = plt.subplots(1,2,figsize=(15,5))

for j in range(1,5):
    autovalor = ["10", "10-e", "5", "2", "1"][j]
    axs[0].errorbar(epsilons, erroresPromedios[j], yerr=erroresDesviaciones[j], fmt='o',
                    capsize=5, elinewidth=2, #markerfacecolor='red',
                    markeredgecolor='black', markersize=8, label='Errores, autovalor ' + autovalor)
    axs[1].errorbar(epsilons, pasosParaConvergerPromedios[j], yerr=pasosParaConvergerDesviaciones[j], fmt='o',
                    capsize=5, elinewidth=2, #markerfacecolor='green',
                    markeredgecolor='black', markersize=8, label='Pasos de Convergencia, autovalor ' + autovalor)

axs[0].set_xscale('log')
axs[0].set_yscale('log')
    #ax_dict["e"+str(j)].tick_params(axis='x', labelsize=8)
    #ax_dict["e"+str(j)].tick_params(axis='y', labelsize=8)
axs[0].set_xlabel('Epsilon')#, fontsize=10)
axs[0].set_ylabel('Promedio de Errores')#, fontsize=10)
axs[0].set_title('Promedio de Errores en función de Epsilon por autovalor')#, fontsize=13)
axs[0].grid(True)
axs[0].legend()

axs[1].set_xscale('log')
axs[1].set_yscale('linear')
    #ax_dict["p"+str(j)].tick_params(axis='x', labelsize=8)
    #ax_dict["p"+str(j)].tick_params(axis='y', labelsize=8)
axs[1].set_xlabel('Epsilon')#, fontsize=10)
axs[1].set_ylabel('Promedio de Pasos')#, fontsize=10)
axs[1].set_title('Promedio de Pasos de Convergencia en función de Epsilon por autovalor')#, fontsize=13)
axs[1].grid(True)
axs[1].legend(loc=(0.02,0.2))

"""--------------------------------------------------------

Data import
"""

!wget -O wiki_movie_plots_deduped_sample.csv https://www.dropbox.com/scl/fi/xnztguety6brdy7t2lfge/wiki_movie_plots_deduped_sample.csv?rlkey=7m867bh7ruilw66qlh7ozl9g4&dl=1

df = pd.read_csv("wiki_movie_plots_deduped_sample.csv")
df

"""Classification for trainng and testing"""

tokens = np.hstack(df["tokens"].apply(lambda x: x.split()).values)

pd.Series(tokens).value_counts().head(50).sort_values().plot(kind='barh', figsize=(10,10))

def count_unique_tokens(df, cant = 1000):
  unique_tokens = pd.Series(tokens).value_counts().index[:cant].values
  unique_tokens_dict = dict(zip(unique_tokens, range(len(unique_tokens))))
  X = np.zeros((len(df), len(unique_tokens)), dtype=int)
  for i, row in df.iterrows():
      for token in row["tokens"].split():
          if unique_tokens_dict.get(token,False)!=False:
              X[i, unique_tokens_dict[token]] += 1
  return X, unique_tokens

X, unique_tokens = count_unique_tokens(df)
X.shape

def extraer_tokens(data_frame, tokens_dict, n=1000):
    X = np.zeros((len(data_frame), len(tokens_dict.keys())), dtype=int)
    for i in range(len(data_frame)):
        for token in data_frame.iloc[i]["tokens"].split():
            if tokens_dict.get(token,False)!=False:
                X[i, tokens_dict[token]] += 1
    return X

def distancia_coseno(test,train):
    norm_test = np.linalg.norm(test, axis=1)
    norm_train = np.linalg.norm(train, axis=1)
    #tuve que hacer esto y quedarme con el max ese para evitar dividir por 0
    norm_test_inv = np.diag(np.array([1/np.max([1e-24,x]) for x in norm_test]))
    norm_train_inv = np.diag(np.array([1/np.max([1e-24,x]) for x in norm_train]))
    distance_cosine = norm_test_inv @ test @ train.T @ norm_train_inv
    return 1 - distance_cosine

def knn(test, train, k, n=1000):

    tokens = np.hstack(train["tokens"].apply(lambda x: x.split()).values)
    unique_tokens = pd.Series(tokens).value_counts().index[:n].values
    unique_tokens_dict = dict(zip(unique_tokens, range(len(unique_tokens))))

    X_test = extraer_tokens(test, unique_tokens_dict, n)
    X_train = extraer_tokens(train, unique_tokens_dict, n)

    distancias = distancia_coseno(X_test, X_train)
    return np.argsort(distancias)[:,:k]


def predictor(test, train, k, n=1000):
    nearest_neighbor = knn(test, train, k, n)
    genres_neihbors = [[train.iloc[i]["Genre"] for i in m] for m in nearest_neighbor]

    res = []
    for i in range(len(test)):
        actual_genre = test.iloc[i]["Genre"]
        prediction = mode(genres_neihbors[i])
        res.append ({
            "actual_genre": actual_genre,
            "prediction": prediction})
    return res

def accuracy(predictions):
  correct = 0
  for i in range(len(predictions)):
    if predictions[i]["actual_genre"] == predictions[i]["prediction"]:
      correct += 1
  return correct / len(predictions)

def split_data(data_train):
    genre1 = data_train[data_train['Genre'] == 'science fiction']
    genre2 = data_train[data_train['Genre'] == 'crime']
    genre3 = data_train[data_train['Genre'] == 'romance']
    genre4 = data_train[data_train['Genre'] == 'western']

    len_split = len(genre1) // 4
    split1 = pd.concat([genre1[:len_split], genre2[:len_split], genre3[:len_split], genre4[:len_split]])
    split2 = pd.concat([genre1[len_split:(2*len_split)], genre2[len_split:(2*len_split)], genre3[len_split:(2*len_split)], genre4[len_split:(2*len_split)]])
    split3 = pd.concat([genre1[2*len_split:(3*len_split)], genre2[2*len_split:(3*len_split)], genre3[2*len_split:(3*len_split)], genre4[2*len_split:(3*len_split)]])
    split4 = pd.concat([genre1[3*len_split:], genre2[3*len_split:], genre3[3*len_split:], genre4[3*len_split:]])

    return [split1, split2, split3, split4]

def cross_validation(data_train, k_values=[5], n=1000):
    splits = split_data(data_train)
    accuracies = {}
    for k in k_values:
        accuracies[k] = []
        for j in range(4):
            test_fold = splits[j]
            train_fold = pd.concat([splits[(j+1)%4],splits[(j+2)%4],splits[(j+3)%4]])
            predictions = predictor(test_fold, train_fold, k, n)
            accuracies[k].append(accuracy(predictions))
    return accuracies

train_data_set = X
distances_train = distancia_coseno(train_data_set, train_data_set)

plt.pcolor(distances_train, vmin=distances_train[distances_train>0].min(), vmax=distances_train[distances_train>0].max())
plt.colorbar()

N = train_data_set.shape[0]
plt.hlines(np.arange(N//4,N,N//4),0,N, color="k")
plt.vlines(np.arange(N//4,N,N//4),0,N, color="k")
plt.title("Distancias entre géneros")
plt.yticks(np.arange(N//8,N,N//4), df["Genre"].unique())
plt.xticks(np.arange(N//8,N,N//4), df["Genre"].unique());

cant_por_genero = int(len(distances_train)/4)
distancias_entre_generos = []
for i in range(4):
    distancias = [np.mean(x) for x in distances_train[cant_por_genero*i:cant_por_genero*(i+1),cant_por_genero*i:cant_por_genero*(i+1)]]
    distancias_entre_generos.append(np.mean(distancias))

fig, ax = plt.subplots()

bars = ax.bar(['crime', 'romance', 'science fiction', 'western'], distancias_entre_generos)
ax.bar_label(bars)

ax.set_ylabel('Distancia promedio')
ax.set_xlabel('Género')
ax.set_title('Distancia promedio para peliculas del mismo género')

"""KNN con k=5 y n en [500, 1000, 5000]"""

filtro_test_final = df['split']=='test'
test_final = df[filtro_test_final]
filtro_train_total = df['split']=='train'
train_total = df[filtro_train_total]

valores_n = [500,1000,5000]

acc = {}
for n in valores_n:
    acc[str(n)] = accuracy(predictor(test_final,train_total,5,n=n))

fig, ax = plt.subplots()
fig.suptitle('Porcentaje de aciertos (accuracy)\n usando n palabras \n k=5', fontsize=13, y=1.05)
fig.set_facecolor("lightgray")
bars = ax.bar(acc.keys(), acc.values())
ax.set_xlabel('n')
ax.set_ylabel('accuracy')
for bar in bars:
    yval = bar.get_height()
    ax.text(bar.get_x() + bar.get_width()/2, yval, round(yval, 2), ha='center', va='bottom')

acc = {}
k_values = range(1,100)
for n in valores_n:
    cross_validation_results = cross_validation(train_total,k_values, n=n)
    acc[str(n)] = {}
    for k in k_values:
        acc[str(n)][str(k)] = np.mean(cross_validation_results[k])

fig,ax = plt.subplots(figsize=(8, 6))
fig.subplots_adjust(hspace=1)
fig.suptitle('Porcentaje de aciertos (accuracy)\n del predictor con cantidad de palabras (n)\n en [500,1000,5000] en funcion de k', fontsize=17,y=1.08)
fig.set_facecolor("lightgray")

ax.plot(range(1,100), list(acc[str(valores_n[0])].values()), label="n = 500")
ax.plot(range(1,100), list(acc[str(valores_n[1])].values()), label="n = 1000")
ax.plot(range(1,100), list(acc[str(valores_n[2])].values()), label="n = 5000")
ax.set_xlabel('k', fontsize=13)
ax.set_ylabel('accuracy', fontsize=13)
ax.legend()

def knn_pca(test, train, k, n=1000, p=None):

    tokens = np.hstack(train["tokens"].apply(lambda x: x.split()).values)
    unique_tokens = pd.Series(tokens).value_counts().index[:n].values
    unique_tokens_dict = dict(zip(unique_tokens, range(len(unique_tokens))))

    X_test = extraer_tokens(test, unique_tokens_dict, n)
    X_train = extraer_tokens(train, unique_tokens_dict, n)

    X_train_centered = X_train - X_train.mean(0)
    X_test_centered = X_test - X_train.mean(0)
    C = (X_train_centered.T @ X_train_centered) / (X_train_centered.shape[0]-1)

    #Esto hay que cambiarlo por nuestra implementacion
    autovalores, autovectores = np.linalg.eigh(C)
    V = autovectores[:,::-1]

    if p is None:
        p = len(autovalores)
    else:
        p = min(p, len(autovalores))

    X_test_projected = X_test_centered.dot(V[:,:p])
    X_train_projected = X_train_centered.dot(V[:,:p])

    distancias = distancia_coseno(X_test_projected, X_train_projected)
    return np.argsort(distancias)[:,:k]


def predictor_pca(test, train, k, n=1000, p=None):
    nearest_neighbor = knn_pca(test, train, k, n, p)
    genres_neihbors = [[train.iloc[i]["Genre"] for i in m] for m in nearest_neighbor]

    res = []
    for i in range(len(test)):
        actual_genre = test.iloc[i]["Genre"]
        prediction = mode(genres_neihbors[i])
        res.append ({
            "actual_genre": actual_genre,
            "prediction": prediction})
    return res

def cross_validation_pca(data_train, k_values=[5], n=1000, p=None):
    splits = split_data(data_train)
    accuracies = {}
    for k in k_values:
        accuracies[k] = []
        for j in range(4):
            test_fold = splits[j]
            train_fold = pd.concat([splits[(j+1)%4],splits[(j+2)%4],splits[(j+3)%4]])
            predictions = predictor_pca(test_fold, train_fold, k, n, p)
            accuracies[k].append(accuracy(predictions))
    return accuracies

"""-----------------------------------------------"""

tokens = np.hstack(train_total["tokens"].apply(lambda x: x.split()).values)
unique_tokens = pd.Series(tokens).value_counts().index[:1000].values
unique_tokens_dict = dict(zip(unique_tokens, range(len(unique_tokens))))

X_test = extraer_tokens(train_total, unique_tokens_dict, 1000)

Xcentered = X_test - X_test.mean(0)
C = (Xcentered.T @ Xcentered) / (Xcentered.shape[0]-1)


autovalores, autovectores = powerMethod.eigen(C, 1000, 1000, 1e-20)

fig, ax = plt.subplots(1,2, figsize=(13,5))
fig.suptitle('Cantidad de varianza en función\n de la cantidad de componentes p', fontsize=13, y=1.05)
fig.set_facecolor("lightgray")
t = np.sum(np.array(autovalores))
ax[0].plot(range(1,101),autovalores[:100]/t)
ax[0].scatter(range(1,101),autovalores[:100]/t)

ax[0].plot(range(1,101),np.cumsum(autovalores[:100])/t)
ax[0].scatter(range(1,101),np.cumsum(autovalores[:100])/t)
ax[0].set_ylabel("Porcentaje de varianza")
ax[0].set_xlabel("Cantidad de componentes")
ax[0].grid(True)
ax[0].set_title("Primeras 100 componentes")


ax[1].plot(range(1,11),autovalores[:10]/t)
ax[1].scatter(range(1,11),autovalores[:10]/t)

ax[1].plot(range(1,11),np.cumsum(autovalores[:10])/t)
ax[1].scatter(range(1,11),np.cumsum(autovalores[:10])/t)
ax[1].set_ylabel("Porcentaje de varianza")
ax[1].set_xlabel("Cantidad de componentes")
ax[1].grid(True)
ax[1].set_title("Primeras 10 componentes")

"""#Optimizacion de k y p"""

def knn_raw(test, train, k, n=1000):

    distancias = distancia_coseno(test, train)
    return np.argsort(distancias)[:,:k]

folds = split_data(train_total)
data={}
n=1000
for j in range(4):
    test_fold = folds[j]
    train_fold = pd.concat([folds[(j+1)%4],folds[(j+2)%4],folds[(j+3)%4]])

    tokens = np.hstack(train_fold["tokens"].apply(lambda x: x.split()).values)
    unique_tokens = pd.Series(tokens).value_counts().index[:n].values
    unique_tokens_dict = dict(zip(unique_tokens, range(len(unique_tokens))))

    X_test = extraer_tokens(test_fold, unique_tokens_dict, n)
    X_train = extraer_tokens(train_fold, unique_tokens_dict, n)

    X_train_centered = X_train - X_train.mean(0)
    X_test_centered = X_test - X_train.mean(0)
    C = (X_train_centered.T @ X_train_centered) / (X_train_centered.shape[0]-1)
    autovalores, autovectores = powerMethod.eigen(C, 200, 1000, 1e-20)
    data[j]={}
    for p in range(2,200):
        #p = int(p)
        data[j][p]={}
        test_projected = X_test_centered.dot(autovectores[:,:p])
        train_projected = X_train_centered.dot(autovectores[:,:p])
        for k in range(5,100,10):
            k = int(k)
            k_neighbors = knn_raw(test_projected, train_projected, k)
            genres_neihbors = [[train_fold.iloc[i]["Genre"] for i in m] for m in k_neighbors]
            predictions = []
            for i in range(len(X_test)):
                actual_genre = test_fold.iloc[i]["Genre"]
                prediction = mode(genres_neihbors[i])
                predictions.append ({
                    "actual_genre": actual_genre,
                    "prediction": prediction})
            acc = accuracy(predictions)
            data[j][p][k] = acc

promedios = data[0]
for p in list(data[0].keys()):
    for k in list(data[0][p]):
        promedios[p][k] += data[1][p][k]
        promedios[p][k] += data[2][p][k]
        promedios[p][k] += data[3][p][k]
        promedios[p][k] /= 4

matriz = []
for i in range(len(list(promedios.keys()))):
    p = list(promedios.keys())[i]
    matriz.append([])
    for k in list(promedios[p].keys()):
        matriz[i].append(promedios[p][k])

fig = plt.subplot()
d = fig.imshow(matriz)
fig.set_yticks(range(0,198,10))
fig.set_yticklabels(range(2,200)[::10])
fig.set_xticks(range(10))
fig.set_xticklabels(range(5,100,10))
fig.set_aspect(0.07)
fig.set_title("Porcentaje de aciertos (accuracy)\n según cantidad de vecinos y cantidad\n de valores principales")
fig.set_xlabel("cantidad de vecinos (k)")
fig.set_ylabel("cantidad de valores principales (p)")
plt.colorbar(d)
plt.show()

matriz = np.array(matriz)
max_index = np.argmax(matriz)
max_coordinates = np.unravel_index(max_index, matriz.shape)
p_max = list(promedios.keys())[max_coordinates[0]]
k_max = list(promedios[p_max].keys())[max_coordinates[1]]
print(p_max)
print(k_max)

matriz = []
for i in range(len(list(promedios.keys()))):
    p = list(promedios.keys())[i]
    matriz.append([])
    for k in list(promedios[p].keys()):
        matriz[i].append(promedios[p][k])

n = 1000
tokens = np.hstack(train_total["tokens"].apply(lambda x: x.split()).values)
unique_tokens = pd.Series(tokens).value_counts().index[:n].values
unique_tokens_dict = dict(zip(unique_tokens, range(len(unique_tokens))))

X_test = extraer_tokens(test_final, unique_tokens_dict, n)
X_train = extraer_tokens(train_total, unique_tokens_dict, n)

X_train_centered = X_train - X_train.mean(0)
X_test_centered = X_test - X_train.mean(0)
C = (X_train_centered.T @ X_train_centered) / (X_train_centered.shape[0]-1)
autovalores, autovectores = powerMethod.eigen(C, 35, 1000, 1e-20)

test_projected = X_test_centered.dot(autovectores)
train_projected = X_train_centered.dot(autovectores)

k_neighbors = knn_raw(test_projected, train_projected, 15)
genres_neihbors = [[train_total.iloc[i]["Genre"] for i in m] for m in k_neighbors]
predictions = []
for i in range(len(test_final)):
    actual_genre = test_final.iloc[i]["Genre"]
    prediction = mode(genres_neihbors[i])
    predictions.append ({
        "actual_genre": actual_genre,
        "prediction": prediction})
accuracy(predictions)

"""--------------------------------------------------------------------------------

#Analisis por genero
"""

from sklearn.metrics import confusion_matrix

def confusion_matrix_from_prediction(prediction):
    genero_real = []
    genero_pred = []
    for i in range(len(prediction)):
        genero_real.append(prediction[i]["actual_genre"])
        genero_pred.append(prediction[i]["prediction"])

    cant_por_genero = len(genero_real)/4
    matriz_conf = confusion_matrix(genero_real, genero_pred)#, labels=df["Genre"].unique())
    df_matriz_conf = pd.DataFrame(matriz_conf/cant_por_genero,
                                    index=df["Genre"].unique(),
                                    columns=df["Genre"].unique())
    return df_matriz_conf

res_optimo = confusion_matrix_from_prediction(predictions)

sns.heatmap(res_optimo, annot=True, fmt=".2f", cmap='YlGnBu')
plt.title(f'Porcentaje de aciertos por genero\n usando hiperparametros optimos (k=15, p=35)')
plt.xlabel('Predicciones')
plt.ylabel('Verdaderos Valores')
plt.show()

pred_train_sin_pca = predictor(train_total, test_final,15, n=1000)
res_train_sin_pca = confusion_matrix_from_prediction(pred_train_sin_pca)

sns.heatmap(res_train_sin_pca, annot=True, fmt=".2f", cmap='YlGnBu')
plt.title(f'Porcentaje de aciertos por genero\n sin PCA con k=15')
plt.xlabel('Predicciones')
plt.ylabel('Verdaderos Valores')
plt.show()

"""#Esto nos pareció interesante pero no entró en el informe

##Analisis de las primeras 2 componentes principales
"""

tokens = np.hstack(train_total["tokens"].apply(lambda x: x.split()).values)
unique_tokens = pd.Series(tokens).value_counts().index[:n].values
unique_tokens_dict = dict(zip(unique_tokens, range(len(unique_tokens))))

X = extraer_tokens(train_total, unique_tokens_dict)
Xcentered = X - X.mean(0)
C = (Xcentered.T @ Xcentered) / (Xcentered.shape[0]-1)

#Esto tambien hay que reemplazar por lo nuestro
autovalores, autovectores = np.linalg.eigh(C)
V = autovectores[:,::-1]

Vaprox = V[:,:2]

X_projected = Xcentered.dot(Vaprox)

def genero_a_color(g):
    if g == 'science fiction':
        return 'red'
    elif g == 'crime':
        return 'blue'
    elif g == 'romance':
        return 'green'
    elif g == 'western':
        return 'yellow'

train_df = train_total.reset_index(drop=True)

for genre in train_df['Genre'].unique():
    genre_data = train_df[train_df['Genre'] == genre]
    x = X_projected[genre_data.index]
    plt.scatter(x[:, 0], x[:, 1], color=genero_a_color(genre), label=genre)

plt.xlabel('PC1')
plt.ylabel('PC2')
plt.title('Componentes principales')
plt.legend()
plt.grid(True)
plt.xlim(-5.5,2.5)
plt.ylim(-7,4)
plt.show()

solo_scf = (test_final[test_final['Genre'] == 'science fiction']).reset_index(drop=True)
solo_crime = (test_final[test_final['Genre'] == 'crime']).reset_index(drop=True)
solo_romance = (test_final[test_final['Genre'] == 'romance']).reset_index(drop=True)
solo_western = (test_final[test_final['Genre'] == 'western']).reset_index(drop=True)

print("science fiction: ")
for k in [5,10,15,20,25,100,300,400]:
    acc = accuracy(predictor_pca(solo_scf, train_total, k=k, n=1000, p=2))
    print(f"K: {k}, P: {2}, accuracy {acc}")
print("romance")
for k in [5,10,15,20,25,100,300,400]:
    acc = accuracy(predictor_pca(solo_romance, train_total, k=k, n=1000, p=2))
    print(f"K: {k}, P: {2}, accuracy {acc}")
print("western")
for k in [5,10,15,20,25,100,300,400]:
    acc = accuracy(predictor_pca(solo_western, train_total, k=k, n=1000, p=2))
    print(f"K: {k}, P: {2}, accuracy {acc}")
print("crime")
for k in [5,10,15,20,25,100,300,400]:
    acc = accuracy(predictor_pca(solo_crime, train_total, k=k, n=1000, p=2))
    print(f"K: {k}, P: {2}, accuracy {acc}")

for n in [500, 1000, 5000]:
    acc = cross_validation(train_total, [5,10,15,20,25,30])
    optimal_k = max(acc, key=lambda k: np.mean(acc[k]))
    print(f"Optimal k for n={n}: {optimal_k}")
print(accuracy(predictor(test_final, train_total, optimal_k, n=1000)))

"""#Graficas para dar una interpretacion geometrica de distancia coseno"""

train_random = []
test_random = []
np.random.seed(42)
for _ in range(80):
    train_random.append(np.random.random(2)*2-1)
for _ in range(20):
    test_random.append(np.random.random(2)*2-1)

x_train, y_train = zip(*train_random)
x_test, y_test = zip(*test_random)

figure, ax = plt.subplots()
ax.scatter(x_train, y_train, color="red", label="training data")
ax.scatter(x_test, y_test, color="blue", label="test data")
ax.set_xlabel("x")
ax.set_ylabel("y")
ax.set_title("Datos generados aleatoriamente")
ax.grid(True)
ax.legend()
plt.show()

distancias = distancia_coseno(np.array(test_random),np.array(train_random))

k=5
nearest_neighbor = np.argsort(distancias)[:,:k]

#Se puede cambiar el n para ver otro punto
n=6
figure, ax = plt.subplots()
ax.scatter(x_train, y_train, color="red", alpha=0.3)
ax.scatter(x_test, y_test, color="blue", alpha=0.3)
ax.scatter(x_test[n],y_test[n], color="blue", alpha=1)
x_cercanos = []
y_cercanos = []
for i in range(k):
    x_cercanos.append(x_train[nearest_neighbor[n][i]])
    y_cercanos.append(y_train[nearest_neighbor[n][i]])
ax.scatter(x_cercanos,y_cercanos, color="red", alpha=1)
ax.set_xlabel("x")
ax.set_ylabel("y")
ax.set_title(str(k)+" vecinos mas cercanos en distancia coseno")
ax.grid(True)

#Si se cambia el n capaz hay que cambiar los indices para que quede bien el area
sup = (x_cercanos[0]*1000,y_cercanos[0]*1000)
inf = (x_cercanos[4]*1000,y_cercanos[4]*1000)

x_cono = [0, sup[0], inf[0]]
y_cono = [0, sup[1], inf[1]]
plt.fill(x_cono, y_cono, color='lightblue', alpha=0.4)
ax.set_xlim(-1.1,1.1)
ax.set_ylim(-1.1,1.1)
plt.show()